# üèóÔ∏è Arquitectura del Sistema RAG (Deep Dive)

Este documento explica en detalle el funcionamiento interno, el flujo de datos y las decisiones de dise√±o del proyecto **Simple RAG Modern**. Est√° dise√±ado para entender no solo *c√≥mo* se usa el c√≥digo, sino *qu√©* ocurre "bajo el cap√≥" en un sistema de Inteligencia Artificial Generativa.

---

## 1. Visi√≥n General: El Concepto RAG

**RAG (Retrieval-Augmented Generation)** es una t√©cnica que optimiza la salida de un Modelo de Lenguaje (LLM) como GPT-4, permiti√©ndole consultar una base de conocimientos externa antes de responder.

### La Analog√≠a del Examen
Imagina un examen:
*   **ChatGPT est√°ndar** es un estudiante haciendo un examen de memoria. Si no estudi√≥ el tema, alucinar√° (inventar√°).
*   **RAG** es ese mismo estudiante, pero con un **libro de texto abierto** (tus PDFs) en el escritorio. Cuando le haces una pregunta, primero busca la respuesta en el libro y luego la redacta.

---

## 2. Diagrama de Flujo de Datos

Cuando un usuario hace una pregunta, ocurre el siguiente proceso secuencial:

```mermaid
Usuario -> [Frontend Next.js] -> [Backend FastAPI] -> [OpenAI Embeddings] -> [ChromaDB] -> [LangChain] -> [GPT-4] -> Usuario
```

### Fase A: Ingesta (Cuando subes un PDF)

1.  **Carga (`Loader`)**: El sistema lee el archivo binario PDF y extrae el texto plano.
2.  **Fragmentaci√≥n (`Splitting`)**: El texto se divide en bloques peque√±os (*chunks*) de 1000 caracteres con un solapamiento (*overlap*) de 200 caracteres.
    *   *¬øPor qu√©?* Para preservar el contexto sem√°ntico. Si una frase importante se corta a la mitad, el solapamiento asegura que el siguiente bloque la tenga completa.
3.  **Vectorizaci√≥n (`Embedding`)**: Cada bloque de texto se env√≠a a OpenAI, que devuelve un **Vector** (una lista de 1536 n√∫meros).
    *   *Concepto Clave*: Este vector representa el **significado** del texto. Textos con significados similares tendr√°n vectores matem√°ticamente cercanos.
4.  **Indexado**: Se guardan en **ChromaDB** los pares: `{ Vector, Texto Original }`.

### Fase B: Consulta (Cuando chateas)

1.  **Input**: El usuario pregunta: *"¬øQui√©n es el protagonista?"*.
2.  **Embedding de la Pregunta**: La pregunta se convierte tambi√©n en un vector num√©rico.
3.  **B√∫squeda Sem√°ntica (Retrieval)**:
    *   ChromaDB compara el vector de la pregunta con los millones de vectores de los documentos.
    *   Utiliza **Similitud Coseno** para encontrar los "vecinos m√°s cercanos".
    *   Recupera los 3 fragmentos de texto m√°s relevantes.
4.  **Prompt Engineering (Augmentation)**:
    *   El Backend construye un mensaje invisible para el usuario. Inserta los 3 fragmentos recuperados dentro de una instrucci√≥n para el LLM.
    *   *Estructura del Prompt*: "Usa ESTA informaci√≥n [Fragmento 1, 2, 3] para responder a ESTA pregunta [Input Usuario]".
5.  **Generaci√≥n**: El LLM (GPT-4o-mini) lee los fragmentos y genera una respuesta en lenguaje natural basada **estrictamente** en la evidencia proporcionada.

---

## 3. Componentes T√©cnicos Detallados

### üß† El Cerebro: LangChain
LangChain act√∫a como el orquestador. Es el framework que conecta las piezas. En este proyecto, utiliza `create_retrieval_chain` para automatizar el flujo de:
1.  Ir a la base de datos.
2.  Traer documentos.
3.  Pegarlos en el prompt.
4.  Llamar a OpenAI.

### üóÑÔ∏è La Memoria: ChromaDB
Chroma es una base de datos **Vectorial**.
*   **¬øD√≥nde vive?**: En este MVP, vive en la memoria RAM del contenedor Docker (backend). Es ef√≠mera.
*   **Funci√≥n**: Permite b√∫squedas por "significado" y no por "palabras clave".
    *   *B√∫squeda cl√°sica*: Si buscas "Coche", busca la palabra "Coche".
    *   *B√∫squeda vectorial*: Si buscas "Coche", encuentra "Autom√≥vil", "Veh√≠culo", "Ferrari", porque matem√°ticamente est√°n cerca en el espacio latente.

### üåê El Cuerpo: FastAPI + Docker
*   **FastAPI**: Expone los endpoints REST (`/chat`, `/upload`). Es as√≠ncrono y muy r√°pido.
*   **Docker**: Empaqueta todo el entorno.
    *   El contenedor `backend` tiene Python instalado y todas las librer√≠as de IA.
    *   El contenedor `frontend` tiene Node.js y el servidor de Next.js optimizado.
    *   `docker-compose` crea una red virtual privada donde ambos contenedores pueden hablarse entre s√≠, pero expone los puertos 3000 y 8000 a tu m√°quina host.

---

## 4. Glosario de T√©rminos IA

*   **Embeddings**: Representaci√≥n num√©rica de texto (listas de flotantes). Es el idioma que entienden las m√°quinas para comparar significados.
*   **Chunks**: Fragmentos de texto. Los LLMs tienen un l√≠mite de memoria (ventana de contexto), por lo que no podemos enviarles libros enteros. Les enviamos chunks relevantes.
*   **Alucinaci√≥n**: Cuando un LLM inventa informaci√≥n. RAG reduce esto dr√°sticamente al obligar al modelo a usar fuentes ("Grounding").
*   **Temperature**: Un par√°metro del LLM (configurado a 0 en este proyecto).
    *   `0`: El modelo es determinista, aburrido y preciso. (Ideal para RAG).
    *   `1`: El modelo es creativo y aleatorio.

---

## 5. Gu√≠a de Extensi√≥n (Futuro)

Para llevar este proyecto al siguiente nivel (Nivel Producci√≥n), se deber√≠a:
1.  **Persistencia**: Configurar ChromaDB para guardar datos en disco, de modo que no se borren al reiniciar Docker.
2.  **Memoria de Chat**: Actualmente cada pregunta es independiente. Se puede a√±adir `ConversationBufferMemory` en LangChain para que el bot recuerde preguntas anteriores.
3.  **Streaming**: Hacer que el texto aparezca letra por letra (como ChatGPT) en lugar de esperar toda la respuesta de golpe.
